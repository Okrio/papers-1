\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
%\usepackage[nonatbib]{nips_2017}

\usepackage{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{tikz,placeins}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{mathrsfs}
\usepackage{algpseudocode}
%\usepackage[caption = false]{subfig}
\usepackage{amssymb,amsmath,bm}
\usetikzlibrary{arrows,decorations.markings}
\tikzstyle{every picture}+=[font=\rmfamily\it\bfseries\large]

\graphicspath{{../figures/}}

\title{Enhancement of Speech Recognition Results Using Server-side
Post-processing}

%\renewcommand{\vec}[1]{\mathbf{#1}}

\newcommand{\chanwcom}{Chanwoo Kim}
\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\renewcommand{\floatpagefraction}{0.85}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  David S.~Hippocampus\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}

% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
TODO(chanw.com) Revise the abstract.
This paper proposes a new end-to-end speech recognition system referred to
as Feedback Sequence Classifier (FSC). Recently, end-to-end speech
recognition systems have been gaining more attention from researchers and
several different structures have been proposed. Typical examples  
include Connectionist Temporal Classification (CTC), Recurrent Neural Network
  Transducer (RNN-T), and Sequence-to-Sequence Modeling using the attention
mechanism. The CTC-based approaches are using the CTC loss assuming the
conditional independence property. Although the structure of CTC is simpler
than other end-to-end systems, the performance is worse due to this conditional
independence assumption. RNN-T and attention-based approaches have distinct
components such as the prediction network  and encoder in RNN-T, and
the encoder, the decoder, and the attention layer in the attention-based model.
Compared to these models, FSC has a homogeneous structure from the bottom to 
the top layer. LSTM and max-pooling layer are repeated followed by the top
softmax layer. The embedded softmax output is fed back as input of each RNN
layer. The training starts with flat initialization and alignment is made
after every epoch. The loss is the Cross Entropy (CE) loss using this 
alignment result. In our experimental results, FSC algorithm has shown 
better performance than more complicated attention-based end-to-end speech
recognition system. Another major advantage is training is significantly
faster.
\end{abstract}

\section{Introduction}


\section{Review on frame-synchronous speech recognition}

Let us suppose that the sequence of the text stream is $T$  
\begin{align}
  X
\end{align}

A speech recognizers maps a sequence of input acoustic features into a sequence 
of text labels  \cite{r_prabhavalkar_interspeech_2017_00}. 
Mathematically, this model is 
represented by the following transformation $\mathcal{T}$ 
\cite{a_graves_icml_2006_00}:
\begin{align}
  \mathcal{T} : \left(\mathbb{R}^d\right)^{*} \rightarrow \mathbb{V}^{*}
	\label{eq:transformation}
\end{align}
where the symbol ${*}$ denotes a sequence, $d$ is the dimension of the input 
feature. $\mathbb{V}$ in \eqref{eq:target_label} is the set of output
labels, which may be graphemes 
\cite{j_chorowski_nips_2015_00,
w_chan_icassp_2016_00},
subword units \cite{
a_zeyer_interspeech_2018_00,
c_chiu_icassp_2018_00},
 and words \cite{h_soltau_interspeech_2017_00}.

 

\section{The structure of the Server-side Post Processing System}

Fig. \ref{fig:entire_system} shows the entire structure of the on-device ASR
system and the server-side post processing system. In this figure,  
the two top level blocks on the left and the right hand sides correspond
to {\it speech recognition on the local device} and 
{\it post processing on the server} respectively.  The symbols used in this 
figure is defined as follows:
%
\begin{align}
  \vec{x}[m]:& \quad \text{Acoustic feature vector from speech.}  \nonumber \\
  \hat{y}_o[l]:& \quad \text{The frame-synchronous hypothesis from the
  on-device speech recognizer.}   \nonumber \\
       & \quad \hat{y}_o[l] \in \mathbb{V}.  \nonumber \\
  \hat{y}_o[0:l+1]  : & \quad \text{The sequence of $y[l']$ where } 0 \le l'
  \le l. \quad  \hat{y}_o[l'] \in \mathbb{V}.  \nonumber \\ 
  y_p[l]:& \quad \text{The frame-synchronous hypothesis for post-processing.}
  \quad \hat{y}_p[l] \in \mathbb{V}.   \nonumber \\
  W_i: & \quad \text{The word sequence. } W_i \in  \mathbb{D}.  \nonumber 
\end{align}
where $\mathbb{V}$ is the set of grapheme symbols, and $\mathbb{D}$ is the
set of words. In Fig. \ref{fig:entire_system}, there are two top level
blocks. The left and right blocks correspond to processing on the local
device and the server side post processing respectively.



\subsection{Label Likelihood Calculation}
To perform decoding described in Section \ref{sec:decoding}, 
we need to calculate likelihood $P(\hat{y}_o[0:l+1] | y_p[l])$
where $\hat{y}_o[0:l+1]$ is the sequence of the frame-synchronous 
label hypothesis.
Using the Bayes rule, this likelihood is expressed in the following form:
\begin{align}
  P(\hat{y}_o[0:l+1] \big| y_p[l]) 
    = \frac{P(y_p[l] \big| \hat{y}_o[0:l+1])
        P\left(\hat{y}_o[0:l+1] \right)}{P(y_p[l])}.
  \label{eq:likelihood_prob}
\end{align}
To calculate the above equation, the following three terms need to be
calculated:
\begin{align}
  P(y_p[l] \big| \hat{y}_o[0:l+1]): & \text{ A posterior probability of } y_p[l]
    \text{ given the frame-synchronous label sequence } \nonumber \\ 
    & \hat{y}_o[0:l+1].
    \nonumber \\
  P\left(\hat{y}_o[0:l+1] \right): & 
    \text{ A probability of the sequence of frame-synchronous labels } \hat{y}_o[0:l+1].
      \nonumber \\
  P(y_p[l]): & \text{ A prior probability of the label } y_p[l] \in \mathbb{V}.
  \label{eq:prob_components_likelihood}
\end{align}

To calculate $P\left(\hat{y}_o[0:l+1] \right)$ in \eqref{eq:likelihood_prob}, 
we use a either statistical n-gram or a RNN based Language Model (LM). 
One notable difference of the LM used for this purpose compared 
to a conventional LM is that this LM must be a frame-synchronous LM.
and must be trained from frame-synchronous label corpus. 
Thus, we first need to align transcribed speech corpus to obtain frame-synchronous 
labels and train LM from this corpus. 
Finally, we use this LM to calculate $P\left(\hat{y}_o[0:l+1] \right)$.

Instead of calculating the likelihood of the whole sequence 
$\hat{y}_o[0:l+1]$ described above, a simpler alternative is to ignore
the past values of the original hypothesis $\hat{y}_o[0:l+1]$ and only to
consider the current one $\hat{y}_o[l]$. Under this simplification,
the likelihood in \eqref{eq:likelihood_prob} may be simplified as follows:
\begin{align}
  P(\hat{y}_o[l] \big| y_p[l]) 
    = \frac{P (y_p[l] \big| \hat{y}_o[l]) P(\hat{y}_o[l])}{P(y_p[l])} 
\end{align}

\subsection{Decoding}
\label{sec:decoding}
The structure of the post-processing decoder is depicted in Fig.
\ref{fig:post_processing_decoder}. Using the dictionary and the
language model, the hypothesis word sequence $W_i$ is obtained
from the posterior probability in \eqref{eq:post_prob}.




\begin{figure}
  \centering
    \centering
    \resizebox{130mm}{!}{
      \input{../figures/hybrid_on_device_cloud_asr.tex}
    }
    \caption {
      The structure of an entire system consisting of the on-device speech 
      recognition and the server-side post processing. 
      \label{fig:entire_system}
    }
\end{figure}


\begin{figure}
  \centering
    \centering
    \resizebox{55mm}{!}{
      \input{../figures/asr_system.tex}
    }
    \caption {
      The on-device Automatic Speech Recognition (ASR) system.
      \label{fig:asr_system}
    }
\end{figure}


\begin{figure}
    \centering
    \resizebox{30mm}{!}{
      \input{../figures/label_posteior_prob_estimation.tex}
    } 
    \caption {
      Estimation of the label posterior probability
      \label{fig:label_posterior_prob_estimation}
    }
\end{figure}


\begin{figure}
    \centering
    \resizebox{70mm}{!}{
      \input{../figures/post_processing_decoder.tex}
    } 
    \caption {
      The post-processing decoder for generating the word sequence $W_i$
      from the label likelihood $P(\hat{y}_o[0:l+1] | \hat{y}_p[l])$ in \eqref{eq:post_prob}, 
      using the dictionary and the language model.
      \label{fig:post_processing_decoder}
    }
\end{figure}


\section{Introduction}
Recently, there has been tremendous improvements in speech recognition
systems fueled by advances in deep neural networks
\cite{
  Yu2013FeatureLearningDNN, 
  g_hinton_ieee_signal_processing_mag_2012,
  Seltzer2013DNNAurora4, 
  t_sainath_taslp_2017_00,
  t_sainath_book_chapter_2017_00,
  v_vanhoucke_nips_workshop_2011_00}.
TODO(chanw.com) Describes history about starting using DNN, LSTM.


\section{Review on sequence-to-sequence speech recognition algorithms}

In this section, we review well-known {\it sequence-to-sequence} algorithms
used in speech recognition.
\cite{
j_chorowski_nips_2015_00, 
a_graves_corr_2012_00, 
y_he_icassp_2019_00,
r_prabhavalkar_interspeech_2017_00}. 
where $M$ is the number of frames in the input feature sequence,
$L$ is the number of labels in the output target sequence,
 and $\mathbb{V}$ in \eqref{eq:target_label} is the set of outputs.
Note that the sequence index in \eqref{eq:input_seq} is a frame
index ({\it represented by} [m]), 
whereas the sequence index in \eqref{eq:target_label} is 
a label index ({\it represented by a subscript} $l$). 
Depending on whether the neural network generates
the inference output for every frame or not, there are following
categories:



With widespread adoption of voice assistant speakers
such as Google Home, Amazon Alexa
\cite{B_Li_INTERSPEECH_2017_1, C_Kim_INTERSPEECH_2017_1},
far-field speech recognition has become very important.
Recently, approaches using data augmentation
has been gaining popularity for far-field speech recognition
\cite{R_Lippmann_icassp_1987_1,
c_kim_interspeech_2018_00, w_hartmann_interspeech_2016_00}.

TODO(chanw.com) Briefly describes CTC, RNN-T, and attention-based model.

Recently, it has been observed that if 
{\it sequence-to-sequence end-to-end} ASR systems are trained on sufficiently 
large amounts of acoustic training data, they can outperform
conventional HMM-DNN/RNN hybrid systems \cite{c_chiu_icassp_2018_00, 
c_kim_interspeech_2019_00}.
These complete end-to-end systems have started surpassing the performance of
the conventional WFST-based decoders with a improved training strategy
such as label smoothing, Minimum Word Error Rate(MWER) training, 
learning rate warming-up,
and a better choice of target unit such as Byte Pair Encoded (BPE) 
\cite {r_sennrich_acl_2016_00} subword units.

TODO(chanw.com) Briefly describes the motivation of FSC.



\subsection{Label-synchronous inference}
In this class of models, the neural network generates the inference output 
$\widehat{\vec{y_l}}$ 
only when a new label is expected. The loss function 
$\mathbb{L}_{\text{ls}}\left(\mathcal{X}[0:M], \mathcal{Y}_{0:L}\right)$ is 
defined as follows:
   \begin{align}
     \mathbb{L}_{\text{ls}}\left(\mathcal{X}[0:M], 
        \mathcal{Y}_{0:L}\right) 
        = \sum_{l=0}^{L-1} 
          \sum_{v=0}^{V-1}
          \left(\vec{y}_l\right)_v  \log ( 
             \widehat{\vec{y_l}} )_{v}
          \label{eq:label_synchronous_ce_loss} 
   \end{align}
where $V$ is the number of output labels in the set $\mathbb{V}$ 
mentioned in \eqref{eq:target_label}, and ${(\vec{y}_l)}_v$  
and ${(\widehat{\vec{y_l}})}_{v}$ are the $v$-th element of the 
ground-truth vector $\vec{y}_l$ and the estimated vector by the 
neural network ${\widehat{\vec{y_l}}}$.


\subsection{Frame-synchronous inference}
\label{sec:frame_synchronous_inference}
In this class of inference,  we obtain
the temporal boundaries of each label in \eqref{eq:target_label}.
In the frame-wise Cross Entropy (CE) training 
\cite{g_hinton_ieee_signal_processing_mag_2012_00, 
C_Kim_INTERSPEECH_2017_1, 
B_Li_INTERSPEECH_2017_1}, {\it Viterbi alignment}  
\cite{
x_huang_prentice_hall_2001_00,
l_r_rabiner_proceedings_of_ieee_1989_00} is usually employed. 
The FSC algorithm presented in this paper also obtains
label boundaries using the {\it N-best alignment} which will
be explained in Sec. \ref{sec:n_best_alignment}.

Using these alignment algorithms, 
we obtain corresponding {\it frame-synchronous} label sequences 
$\vec{z}[m] \in \mathbb{V}$ for each frame index $m$. This
latent label sequences are often called a {\it path}.
  \begin{align}
    \mathcal{Z}[0:M]  =  
      \left\{\vec{z}[m]  \Big| 0 \le m \le M-1, \; \vec{z}[m] \in \mathbb{V} \right\}, 
    \label{eq:latent_label_seq}
  \end{align}
Fig. \ref{fig:path_alignment} shows one such example. 

The loss function in the frame-synchronous inference case
$\mathbb{L}_{\text{fs}}\left(\mathcal{X}[0:M], \mathcal{Y}_{0:L}\right)$ is 
defined as follows:
\begin{align}
  \mathbb{L}_{\text{fs}}\left(\mathcal{X}[0:M], \mathcal{Z}[0:M]\right) 
      = \sum_{l=0}^{M-1} 
          \sum_{v=0}^{V-1}  
            (\vec{z}[m])_v  
            \log (\widehat{\vec{z}}[m])_v  
        \label{eq:frame_synchronous_ce_loss} 
\end{align}


 In Fig. 
\ref{fig:ctc_diagram} $\sim$ \ref{fig:attention_diagram}, we show
block diagrams of CTC \cite{a_graves_icml_2006_00}, 
RNN-T \cite{a_graves_corr_2012_00, a_graves_icassp_2013_00}, and 
the attention-based model 
\cite{
j_chorowski_nips_2015_00,
w_chan_icassp_2016_00} respectively. CTC in Fig. \ref{fig:ctc_diagram} 
and RNN-T in Fig. \ref{fig:rnn_t_diagram} are {\it frame-synchronous}, 
which means that the ASR system generates the output target for each 
input frame.


\subsubsection{Connectionist Temporal Classification}
\label{sec:ctc}

Fig. \ref{fig:ctc_diagram} shows the entire structure of the 
Connectionist Temporal Classification (CTC) 
\cite{a_graves_icml_2006_00}. In CTC, we use the following CTC loss
\cite{a_graves_icml_2006_00, y_he_icassp_2019_00}:
\begin{align}
  P\left(\mathcal{Y}_{0:L} | \mathcal{X}[0:M]\right) = 
    \sum_{\mathcal{Z}_{0}^{M-1} \in 
      \mathcal{A}_{CTC} 
        \left(\mathcal{X}_{0}^{M-1}, \mathcal{Y}_{0}^{L-1} \right)}
          \Pi_{m=0}^{M-1}
            P\left(\vec{z}[m] | \mathcal{X}_{0}^{M-1} \right),
\end{align}
where $\mathcal{A}_{CTC} \left(\mathcal{X}_{0}^{M-1}, 
\mathcal{Y}_{0}^{L-1} \right)$ correspond to frame-level alignments
of length $M$ such that removing blanks and repeated symbols from 
$\mathcal{Z}_{0}^{M-1}$ yields $\mathcal{Y}_{0}^{L-1}$.

To obtain derivatives required for back-propagation, CTC employs 
the forward-backward algorithm.



\subsubsection{RNN-Transducer}
\label{sec:rnn_t}

Fig. \ref{fig:rnn_t_diagram} shows the entire structure of the  
RNN-T structure. The loss function used in RNN-T is given as follows:
\begin{align}
  P\left(\mathcal{Y}_{0:L} | \mathcal{X}[0:M]\right) = 
    \sum_{\mathcal{Z}_{0}^{M-1} \in 
      \mathcal{A}_{CTC} 
        \left(\mathcal{X}_{0}^{M-1}, \mathcal{Y}_{0}^{L-1} \right)}
          \Pi_{m=0}^{M-1}
            P\left(\vec{z}[m] | \mathcal{X}_{0}^{M-1} \right),
\end{align}


\begin{figure}
  \centering
  \begin{subfigure}[b]{0.60\textwidth}
    \centering
    \resizebox{78mm}{!}{
      \input{../figures/rnn_t_diagram.tex}
    }
    \caption {
      RNN-T Model.
      \label{fig:rnn_t_diagram}
    }
  \end{subfigure}

  \begin{subfigure}[b]{0.55\textwidth}
    \centering
    \resizebox{53mm}{!}{
      \input{../figures/attention_diagram.tex}
    }
    \caption {
      Attention-based Model.
      \label{fig:attention_diagram}
    }
  \end{subfigure}
  \caption{
    Comparison of block diagrams of different sequence-to-sequence speech
    recognition approaches.
    The proposed Feedback Sequence Classifier (FSC) is shown in 
    Fig. \ref{fig:fsc_diagram}.
  }

\end{figure}




\section{Experimental Results}
\label{sec:experimental_results}


\subsubsection*{Acknowledgments}



\small

\clearpage
\newpage
\bibliographystyle{plain}
\bibliography{../../common_bib_file/common_bib_file}



\end{document}
