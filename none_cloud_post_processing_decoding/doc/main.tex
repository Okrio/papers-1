\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
%\usepackage[nonatbib]{nips_2017}

\usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{tikz,placeins}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{mathrsfs}
\usepackage{algpseudocode}
%\usepackage[caption = false]{subfig}
\usepackage{amssymb,amsmath,bm}
\usetikzlibrary{arrows,decorations.markings}
\tikzstyle{every picture}+=[font=\rmfamily\it\bfseries\large]

\graphicspath{{../figures/}}

\title{Enhancement of Speech Recognition Results Using Server-side
Post-processing}

%\renewcommand{\vec}[1]{\mathbf{#1}}

\newcommand{\chanwcom}{Chanwoo Kim}
\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\renewcommand{\floatpagefraction}{0.85}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Chanwoo Kim  \\
  Samsung Research \\
  Seoul, Korea, 06765 \\
  \texttt{chanw.com@samsung.com}  \\
  \And
  Kyungmin Lee  \\
  Samsung Research \\
  Seoul, Korea, 06765 \\
  \texttt{k-m-lee@samsung.com} \\
  \And
  Eunhyang Kim  \\
  Samsung Research \\
  Seoul, Korea, 06765 \\
  \texttt{sc.ehkim.jin@samsung.com} \\
  \And
  Kwangyoun Kim \\
  Samsung Research \\
  Seoul, Korea, 06765 \\
  \texttt{sc.ehkim.jin@samsung.com} \\
  \And
  Dhananjaya Gowda \\
  Samsung Research \\
  Seoul, Korea, 06765 \\
  \texttt{d.gowda@samsung.com} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}

% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
TODO(chanw.com) Revise the abstract.
This paper proposes a new end-to-end speech recognition system referred to
as Feedback Sequence Classifier (FSC). Recently, end-to-end speech
recognition systems have been gaining more attention from researchers and
several different structures have been proposed. Typical examples  
include Connectionist Temporal Classification (CTC), Recurrent Neural Network
  Transducer (RNN-T), and Sequence-to-Sequence Modeling using the attention
mechanism. The CTC-based approaches are using the CTC loss assuming the
conditional independence property. Although the structure of CTC is simpler
than other end-to-end systems, the performance is worse due to this conditional
independence assumption. RNN-T and attention-based approaches have distinct
components such as the prediction network  and encoder in RNN-T, and
the encoder, the decoder, and the attention layer in the attention-based model.
Compared to these models, FSC has a homogeneous structure from the bottom to 
the top layer. LSTM and max-pooling layer are repeated followed by the top
softmax layer. The embedded softmax output is fed back as input of each RNN
layer. The training starts with flat initialization and alignment is made
after every epoch. The loss is the Cross Entropy (CE) loss using this 
alignment result. In our experimental results, FSC algorithm has shown 
better performance than more complicated attention-based end-to-end speech
recognition system. Another major advantage is training is significantly
faster.
\end{abstract}

\section{Introduction}

Recently, there has been tremendous improvements in speech recognition
systems fueled by advances in deep neural networks
\cite{
  Yu2013FeatureLearningDNN, 
  g_hinton_ieee_signal_processing_mag_2012,
  Seltzer2013DNNAurora4, 
  t_sainath_taslp_2017_00,
  t_sainath_book_chapter_2017_00,
  v_vanhoucke_nips_workshop_2011_00}.
These improvements have come about from the shift from Gaussian Mixture Model
(GMM) to the Feed-Forward Deep Neural Networks (FF-DNNs), FF-DNNs
to Recurrent Neural Network (RNN) and in particular the Long Short-Term Memory
(LSTM) networks.
Thanks to these advances, voice assistant devices such as Google Home
\cite{C_Kim_INTERSPEECH_2017_1, B_Li_INTERSPEECH_2017_1}
or Amazon Alexa or Samsung Bixby are being used at many homes and on personal devices.

Recently there has been increasing interest in switching
from the conventional Weighted Finite State Transducer (WFST)
based decoder using an Acoustic Model (AM) and a Language Model (LM)
to a complete end-to-end all-neural speech recognition systems
\cite{w_chan_icassp_2016_00, r_prabhavalkar_interspeech_2017_00,
j_chorowski_nips_2015_00}.
These complete end-to-end systems have started surpassing the performance of
the conventional WFST-based decoders with a very large training
database, a better choice of target unit such as Byte Pair Encoded (BPE)
subword units, and an improved training methodology such as Minimum
Word Error Rate (MWER) training.

Another important aspect of these recent improvements in speech
recognition performance is data.
To build high performance speech recognition systems for
conversational speech, we need to use
a large amount of speech data covering various domains. In
\cite{h_soltau_interspeech_2017_00}, it has been shown
that we need a very large training set ($\sim$125,000 hours of
semi-supervised speech data) to achieve
high speech recognition accuracy for difficult tasks like
video captioning. To train neural networks using such large
amounts of speech data, we usually need
multiple Central Processing Units (CPUs) or Graphics
Processing Units (GPUs) or GPU clusters
\cite{E_Variani_INTERSPEECH_2017_01, p_goyal_arxiv_2018_00}.




\section{Overview of the Enhancement by Post-Processing}

\begin{figure}
  \centering
    \centering
    \resizebox{130mm}{!}{
      \input{../figures/hybrid_on_device_cloud_asr.tex}
    }
    \caption {
      The structure of an entire system consisting of the on-device speech 
      recognition and the server-side post processing. 
      \label{fig:entire_system}
    }
\end{figure}


Fig. \ref{fig:entire_system} shows the entire structure of the on-device ASR
system and the server-side post processing system. In this figure,  
the two top level blocks on the left and the right hand sides correspond
to {\it speech recognition on the local device} and 
{\it post processing on the server} respectively.  

TODO(chanw.com) Describe the advantage of the server side post-processing.


\begin{subequations}
  \begin{align}
    \text{On-Device Speech Recognition}   : 
        & \quad \{ \, \cdots ,\, \vec{x}[m],     \cdots \, \} \rightarrow  
                \{ \, \cdots ,\, \hat{y}_o[l],   \cdots \, \} \\
    \text{Server-Side Post-Processing} : 
        & \quad \{ \, \cdots ,\, \hat{y}_o[l],   \cdots \, \}  \rightarrow
                \{ \, \cdots ,\, W_i,   \cdots \, \}
  \end{align}
\end{subequations}
where $\vec{x}[m]$ is the speech feature vector with $m$ being the 
frame index and $\hat{z}_o[l]$ is the frame-synchronous hypothesis 
from the speech recognizer with $l$ being the frame index. More details
about the frame-synchronous hypothesis will be covered in Sec. 
\ref{sec:frame_synchronous_hypothesis}. The reason for using a different
frame index $l$ for $\hat{y}_o[l]$ is that the frame period in hypothesis
generation may be different from the frame period in feature extraction.
As will be discussed in Sec \ref{sec:frame_synchronous_hypothesis}, 
the sequence of $\hat{y}_o[l]$ is a stream of graphemes. 

When the server-side post processing is needed, the sequence of  
of frame synchronous hypothesis 





Before describing structures in detail in Sec.
\ref{sec:frame_synchronous_hypothesis} and in Sec.
\ref{sec:server_side_post_processing}, we will first define notational 
conventions in this section. When representing a sequence consisting of 
periodic elements we will use a bracket symbol to represent elements of
such a sequence. For example, speech features are usually calculated 
periodically at every frame interval (e.g. 10 ms). Thus, we use  
the symbol $\vec{x}[m]$ in
To represent sequences, we will use the bracket symbol
to represent periodic data such as feature vector $\vec{x}[m]$. Usually
speech features are usually sampled at around 10 ms intervals. TODO(chanw.com)
Adds citation. To represent a sequence $\alpha$ starting from the index 
$i$ and ending at $f-1$, we use the notation inspired by the python slice 
notation 
\begin{align}
  \alpha[i:f] = \{ \alpha[m]| i \le m < f \} 
\end{align}
Using this notational convention, we summarize the symbols used in Fig
\ref{fig:entire_system} as follows:
\begin{align}
  \vec{x}[m]:& \quad \text{Acoustic feature vector from speech.}  \nonumber \\
  \hat{y}_o[l]:& \quad \text{The frame-synchronous hypothesis from the
  on-device speech recognizer.}   \nonumber \\
       & \quad \hat{y}_o[l] \in \mathbb{V}.  \nonumber \\
  \hat{y}_o[0:l+1]  : & \quad \text{The sequence of $y[l']$ where } 0 \le l'
  \le l. \quad  \hat{y}_o[l'] \in \mathbb{V}.  \nonumber \\ 
  y_p[l]:& \quad \text{The frame-synchronous hypothesis for post-processing.}
  \quad \hat{y}_p[l] \in \mathbb{V}.   \nonumber \\
  W_i: & \quad \text{The word sequence. } W_i \in  \mathbb{D}.  \nonumber 
\end{align}
where $\mathbb{V}$ is the set of grapheme symbols, and $\mathbb{D}$ is the
set of words. In Fig. \ref{fig:entire_system}, there are two top level


\section{Obtaining frame-synchronous hypothesis}
\label{sec:frame_synchronous_hypothesis}

In this section, we describe how to obtain frame-synchronous  
hypothesis from a speech recognition decoder. The function of a speech
recognizer is to map a sequence of acoustic feature vectors into
a sequence of graphemes or words. This transformation may be represented 
by the following equation:
\begin{align}
  \mathcal{T} : \left(\mathbb{R}^d\right)^{*} \rightarrow \mathbb{V}^{*}
	\label{eq:transformation}
\end{align}

In this section, we review well-known {\it sequence-to-sequence} algorithms
used in speech recognition.
\cite{
j_chorowski_nips_2015_00, 
a_graves_corr_2012_00, 
y_he_icassp_2019_00,
r_prabhavalkar_interspeech_2017_00}. 
where $M$ is the number of frames in the input feature sequence,
$L$ is the number of labels in the output target sequence,
 and $\mathbb{V}$ in \eqref{eq:target_label} is the set of outputs.
Note that the sequence index in \eqref{eq:input_seq} is a frame
index ({\it represented by} [m]), 
whereas the sequence index in \eqref{eq:target_label} is 
a label index ({\it represented by a subscript} $l$). 
Depending on whether the neural network generates
the inference output for every frame or not, there are following
categories:



With widespread adoption of voice assistant speakers
such as Google Home, Amazon Alexa
\cite{B_Li_INTERSPEECH_2017_1, C_Kim_INTERSPEECH_2017_1},
far-field speech recognition has become very important.
Recently, approaches using data augmentation
has been gaining popularity for far-field speech recognition
\cite{R_Lippmann_icassp_1987_1,
c_kim_interspeech_2018_00, w_hartmann_interspeech_2016_00}.

TODO(chanw.com) Briefly describes CTC, RNN-T, and attention-based model.

Recently, it has been observed that if 
{\it sequence-to-sequence end-to-end} ASR systems are trained on sufficiently 
large amounts of acoustic training data, they can outperform
conventional HMM-DNN/RNN hybrid systems \cite{c_chiu_icassp_2018_00, 
c_kim_interspeech_2019_00}.
These complete end-to-end systems have started surpassing the performance of
the conventional WFST-based decoders with a improved training strategy
such as label smoothing, Minimum Word Error Rate(MWER) training, 
learning rate warming-up,
and a better choice of target unit such as Byte Pair Encoded (BPE) 
\cite {r_sennrich_acl_2016_00} subword units.

TODO(chanw.com) Briefly describes the motivation of FSC.



\subsection{Label-synchronous inference}
In this class of models, the neural network generates the inference output 
$\widehat{\vec{y_l}}$ 
only when a new label is expected. The loss function 
$\mathbb{L}_{\text{ls}}\left(\mathcal{X}[0:M], \mathcal{Y}_{0:L}\right)$ is 
defined as follows:
   \begin{align}
     \mathbb{L}_{\text{ls}}\left(\mathcal{X}[0:M], 
        \mathcal{Y}_{0:L}\right) 
        = \sum_{l=0}^{L-1} 
          \sum_{v=0}^{V-1}
          \left(\vec{y}_l\right)_v  \log ( 
             \widehat{\vec{y_l}} )_{v}
          \label{eq:label_synchronous_ce_loss} 
   \end{align}
where $V$ is the number of output labels in the set $\mathbb{V}$ 
mentioned in \eqref{eq:target_label}, and ${(\vec{y}_l)}_v$  
and ${(\widehat{\vec{y_l}})}_{v}$ are the $v$-th element of the 
ground-truth vector $\vec{y}_l$ and the estimated vector by the 
neural network ${\widehat{\vec{y_l}}}$.


\subsection{Frame-synchronous inference}
\label{sec:frame_synchronous_inference}
In this class of inference,  we obtain
the temporal boundaries of each label in \eqref{eq:target_label}.
In the frame-wise Cross Entropy (CE) training 
\cite{g_hinton_ieee_signal_processing_mag_2012_00, 
C_Kim_INTERSPEECH_2017_1, 
B_Li_INTERSPEECH_2017_1}, {\it Viterbi alignment}  
\cite{
x_huang_prentice_hall_2001_00,
l_r_rabiner_proceedings_of_ieee_1989_00} is usually employed. 
The FSC algorithm presented in this paper also obtains
label boundaries using the {\it N-best alignment} which will
be explained in Sec. \ref{sec:n_best_alignment}.

Using these alignment algorithms, 
we obtain corresponding {\it frame-synchronous} label sequences 
$\vec{z}[m] \in \mathbb{V}$ for each frame index $m$. This
latent label sequences are often called a {\it path}.
  \begin{align}
    \mathcal{Z}[0:M]  =  
      \left\{\vec{z}[m]  \Big| 0 \le m \le M-1, \; \vec{z}[m] \in \mathbb{V} \right\}, 
    \label{eq:latent_label_seq}
  \end{align}
Fig. \ref{fig:path_alignment} shows one such example. 

The loss function in the frame-synchronous inference case
$\mathbb{L}_{\text{fs}}\left(\vec{x}[0:M], y_{0:L}\right)$ is 
defined as follows:
\begin{align}
  \mathbb{L}_{\text{fs}}\left(\vec{x}[0:M], z[0:M]\right) 
      = \sum_{l=0}^{M-1} 
          \sum_{v=0}^{V-1}  
            (\vec{z}[m])_v  
            \log (\widehat{\vec{z}}[m])_v  
        \label{eq:frame_synchronous_ce_loss} 
\end{align}


 In Fig. 
\ref{fig:ctc_diagram} $\sim$ \ref{fig:attention_diagram}, we show
block diagrams of CTC \cite{a_graves_icml_2006_00}, 
RNN-T \cite{a_graves_corr_2012_00, a_graves_icassp_2013_00}, and 
the attention-based model 
\cite{
j_chorowski_nips_2015_00,
w_chan_icassp_2016_00} respectively. CTC in Fig. \ref{fig:ctc_diagram} 
and RNN-T in Fig. \ref{fig:rnn_t_diagram} are {\it frame-synchronous}, 
which means that the ASR system generates the output target for each 
input frame.


\subsubsection{Connectionist Temporal Classification}
\label{sec:ctc}

Fig. \ref{fig:ctc_diagram} shows the entire structure of the 
Connectionist Temporal Classification (CTC) 
\cite{a_graves_icml_2006_00}. In CTC, we use the following CTC loss
\cite{a_graves_icml_2006_00, y_he_icassp_2019_00}:
\begin{align}
  P\left(\mathcal{Y}_{0:L} | \mathcal{X}[0:M]\right) = 
    \sum_{\mathcal{Z}_{0}^{M-1} \in 
      \mathcal{A}_{CTC} 
        \left(\mathcal{X}_{0}^{M-1}, \mathcal{Y}_{0}^{L-1} \right)}
          \Pi_{m=0}^{M-1}
            P\left(\vec{z}[m] | \mathcal{X}_{0}^{M-1} \right),
\end{align}
where $\mathcal{A}_{CTC} \left(\mathcal{X}_{0}^{M-1}, 
\mathcal{Y}_{0}^{L-1} \right)$ correspond to frame-level alignments
of length $M$ such that removing blanks and repeated symbols from 
$\mathcal{Z}_{0}^{M-1}$ yields $\mathcal{Y}_{0}^{L-1}$.

To obtain derivatives required for back-propagation, CTC employs 
the forward-backward algorithm.



\subsubsection{RNN-Transducer}
\label{sec:rnn_t}

Fig. \ref{fig:rnn_t_diagram} shows the entire structure of the  
RNN-T structure. The loss function used in RNN-T is given as follows:
\begin{align}
  P\left(\mathcal{Y}_{0:L} | \mathcal{X}[0:M]\right) = 
    \sum_{\mathcal{Z}_{0}^{M-1} \in 
      \mathcal{A}_{CTC} 
        \left(\mathcal{X}_{0}^{M-1}, \mathcal{Y}_{0}^{L-1} \right)}
          \Pi_{m=0}^{M-1}
            P\left(\vec{z}[m] | \mathcal{X}_{0}^{M-1} \right),
\end{align}


\begin{figure}
  \centering
  \begin{subfigure}[b]{0.50\textwidth}
    \centering
    \resizebox{70mm}{!}{
      \input{../figures/rnn_t_diagram.tex}
    }
    \caption {
      RNN-T Model.
      \label{fig:rnn_t_diagram}
    }
  \end{subfigure}
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \resizebox{55mm}{!}{
      \input{../figures/attention_diagram.tex}
    }
    \caption {
      Attention-based Model.
      \label{fig:attention_diagram}
    }
  \end{subfigure}
  \caption{
    Examples of a frame-synchronous decoding in \ref{fig:rnn_t_diagram}
    and a label-synchronous decoding in \ref{fig:attention_diagram}.
  }

\end{figure}



Let us suppose that the sequence of the text stream is $T$  
\begin{align}
  X
\end{align}

A speech recognizers maps a sequence of input acoustic features into a sequence 
of text labels  \cite{r_prabhavalkar_interspeech_2017_00}. 
Mathematically, this model is 
represented by the following transformation $\mathcal{T}$ 
\cite{a_graves_icml_2006_00}:
\begin{align}
  \mathcal{T} : \left(\mathbb{R}^d\right)^{*} \rightarrow \mathbb{V}^{*}
	\label{eq:transformation}
\end{align}
where the symbol ${*}$ denotes a sequence, $d$ is the dimension of the input 
feature. $\mathbb{V}$ in \eqref{eq:target_label} is the set of output
labels, which may be graphemes 
\cite{j_chorowski_nips_2015_00,
w_chan_icassp_2016_00},
subword units \cite{
a_zeyer_interspeech_2018_00,
c_chiu_icassp_2018_00},
 and words \cite{h_soltau_interspeech_2017_00}.

 

\section{The structure of the Server-side Post Processing System}
\label{sec:server_side_post_processing}

Fig. \ref{fig:entire_system} shows the entire structure of the on-device ASR
system and the server-side post processing system. In this figure,  
the two top level blocks on the left and the right hand sides correspond
to {\it speech recognition on the local device} and 
{\it post processing on the server} respectively.  




\subsection{Label Likelihood Calculation}
To perform decoding described in Section \ref{sec:decoding}, 
we need to calculate likelihood $P(\hat{y}_o[0:l+1] | y_p[l])$
where $\hat{y}_o[0:l+1]$ is the sequence of the frame-synchronous 
label hypothesis.
Using the Bayes rule, this likelihood is expressed in the following form:
\begin{align}
  P(\hat{y}_o[0:l+1] \big| y_p[l]) 
    = \frac{P(y_p[l] \big| \hat{y}_o[0:l+1])
        P\left(\hat{y}_o[0:l+1] \right)}{P(y_p[l])}.
  \label{eq:likelihood_prob}
\end{align}
To calculate the above equation, the following three terms need to be
calculated:
\begin{align}
  P(y_p[l] \big| \hat{y}_o[0:l+1]): & \text{ A posterior probability of } y_p[l]
    \text{ given the original frame-synchronous hypothesis } \nonumber \\ 
    & \hat{y}_o[0:l+1].
    \nonumber \\
  P\left(\hat{y}_o[0:l+1] \right): & 
    \text{ A probability of the original frame-synchronous hypothesis } \hat{y}_o[0:l+1].
      \nonumber \\
  P(y_p[l]): & \text{ A prior probability of the label } y_p[l] \in \mathbb{V}.
  \label{eq:prob_components_likelihood}
\end{align}
%
\begin{figure}
    \centering
    \resizebox{40mm}{!}{
      \input{../figures/label_posteior_prob_estimation.tex}
    } 
    \caption {
      Estimation of the label posterior probability
      \label{fig:label_posterior_prob_estimation}
    }
\end{figure}
%
To calculate $P(y_p[l] \big| \hat{y}_o[0:l+1])$ in
\eqref{eq:prob_components_likelihood}, we first train a neural network
to predicting the distribution of $y_p[l]$ given the original 
frame-synchronous hypothesis. Fig. \ref{fig:label_posterior_prob_estimation}
shows the structure of such a neural network stack. To train this
neural network stack, we need to have both the frame-level 
original hypothesis $\hat{y}_o[l]$ and the corresponding the frame-level
ground truth. Since the ground-truth labels are usually not given in the
frame-level format, forced alignment needs to be applied to obtain
the frame-synchronous ground truth.

To calculate $P\left(\hat{y}_o[0:l+1] \right)$ in \eqref{eq:likelihood_prob}, 
we use a either statistical n-gram or a RNN based Language Model (LM). 
One notable difference of the LM used for this purpose compared 
to a conventional LM is that this LM must be a frame-synchronous LM.
and must be trained from frame-synchronous label corpus. 
Thus, we first need to align transcribed speech corpus to obtain frame-synchronous 
labels and train LM from this corpus. 
Finally, we use this LM to calculate $P\left(\hat{y}_o[0:l+1] \right)$.

Instead of calculating the likelihood of the whole sequence 
$\hat{y}_o[0:l+1]$ described above, a simpler alternative is to ignore
the past values of the original hypothesis $\hat{y}_o[0:l+1]$ and only to
consider the current one $\hat{y}_o[l]$. Under this simplification,
the likelihood in \eqref{eq:likelihood_prob} may be simplified as follows:
\begin{align}
  P(\hat{y}_o[l] \big| y_p[l]) 
    = \frac{P (y_p[l] \big| \hat{y}_o[l]) P(\hat{y}_o[l])}{P(y_p[l])} 
\end{align}

\subsection{Decoding}
\label{sec:decoding}
The structure of the post-processing decoder is depicted in Fig.
\ref{fig:post_processing_decoder}. Using the dictionary and the
language model, the hypothesis word sequence $W_i$ is obtained
from the posterior probability in \eqref{eq:post_prob}.





\begin{figure}
  \centering
    \centering
    \resizebox{55mm}{!}{
      \input{../figures/asr_system.tex}
    }
    \caption {
      The on-device Automatic Speech Recognition (ASR) system.
      \label{fig:asr_system}
    }
\end{figure}


\begin{figure}
    \centering
    \resizebox{70mm}{!}{
      \input{../figures/post_processing_decoder.tex}
    } 
    \caption {
      The post-processing decoder for generating the word sequence $W_i$
      from the label likelihood $P(\hat{y}_o[0:l+1] | \hat{y}_p[l])$ in \eqref{eq:post_prob}, 
      using the dictionary and the language model.
      \label{fig:post_processing_decoder}
    }
\end{figure}




\section{Experimental Results}
\label{sec:experimental_results}


\subsubsection*{Acknowledgments}



\small

\clearpage
\newpage
\bibliographystyle{plain}
\bibliography{../../common_bib_file/common_bib_file}



\end{document}
